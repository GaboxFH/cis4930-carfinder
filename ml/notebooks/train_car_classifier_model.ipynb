{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_car_classifier_model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2-DLRZcLBueU"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jvsMqs3BqZI",
        "colab_type": "text"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
<<<<<<< Updated upstream
        "id": "9WlTymnvfRpw",
        "colab_type": "text"
      },
      "source": [
        "If you are using Google Colab, run the cell below and make sure your runtime is using GPU.\n",
        "\n",
        "If you are not, do not run the cell below and update the paths in the code from `/content/drive/My Drive`. Also, if you do not have a GPU to use, comment out the lines that say `cuda`."
=======
        "id": "Q7E3CjaDnqw6",
        "colab_type": "text"
      },
      "source": [
        "#### Decisions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSCfSirvntF7",
        "colab_type": "text"
      },
      "source": [
        "There are many decisions that you can make that will affect how the model will perform.\n",
        "\n",
        "The data:\n",
        "- Train/Test/Split percentages can be changed in the Preparation/Constants section\n",
        "- Data augmentation transforms and pre-process transforms of the data can be changed in `process.py`\n",
        "\n",
        "The model:\n",
        "- Hyperparameters can be changed in the Preparation/Constants section\n",
        "- Which model we base our transfer learning from can be changed in the \"Define Model\" section\n",
        "  - Examples: Resnet34, Resnet50, Resnet101, Densenet121\n",
        "- The final layer of the model can be changed in the \"Define Model\" section\n",
        "    - It should map from number of features to number of classes\n",
        "    - Example:  \n",
        "            torch.nn.Sequential(\n",
        "                torch.nn.Linear(num_features, 1024),\n",
        "                torch.nn.LeakyReLU(),\n",
        "                torch.nn.Linear(1024, 512),\n",
        "                torch.nn.LeakyReLU(),\n",
        "                torch.nn.Linear(512, NUM_CLASSES)\n",
        "            )\n",
        "- The optimizer can be changed in the \"Define Model\" section\n",
        "  - Examples:\n",
        "    - `optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)`\n",
        "    - `optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)`\n",
        "- An optional scheduler can be added in the \"Define Model\" section\n",
        "  - Example:\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                       mode='max',\n",
        "                                                       factor=LR_PLATEAU_FACTOR,\n",
        "                                                       threshold=LR_PLATEAU_THRESHOLD,\n",
        "                                                       patience=LR_PLATEAU_PATIENCE,\n",
        "                                                       verbose=True)\n",
        "    - `scheduler.step(valid_accuracy)` should be called every epoch"
>>>>>>> Stashed changes
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-DLRZcLBueU",
        "colab_type": "text"
      },
      "source": [
        "#### Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WlTymnvfRpw",
        "colab_type": "text"
      },
      "source": [
        "If you are using Google Colab, run the cell below and make sure your runtime is using GPU.\n",
        "\n",
        "If you are not, do not run the cell below and update the paths in the code from `/content/drive/My Drive`. Also, if you do not have a GPU to use, comment out the lines that say `cuda`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdAA4c_kBqMv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "851f8ce9-9991-42bf-97f2-00227b6f9b88"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbK4NTZPBxC7",
        "colab_type": "text"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzXEF2Y-BnGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to import custom python files, append the path to sys here\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive')\n",
        "from util import process\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets, models, transforms\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7E3CjaDnqw6",
        "colab_type": "text"
      },
      "source": [
        "#### Decisions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSCfSirvntF7",
        "colab_type": "text"
      },
      "source": [
        "There are many decisions that you can make that will affect how the model will perform.\n",
        "\n",
        "The data:\n",
        "- Train/Test/Split percentages can be changed in the Preparation/Constants section\n",
        "- Data augmentation transforms and pre-process transforms of the data can be changed in `process.py`\n",
        "\n",
        "The model:\n",
        "- Hyperparameters can be changed in the Preparation/Constants section\n",
        "- Which model we base our transfer learning from can be changed in the \"Define Model\" section\n",
        "  - Examples: Resnet34, Resnet50, Resnet101, Densenet121\n",
        "- The final layer of the model can be changed in the \"Define Model\" section\n",
        "    - It should map from number of features to number of classes\n",
        "    - Example:  \n",
        "            torch.nn.Sequential(\n",
        "                torch.nn.Linear(num_features, 1024),\n",
        "                torch.nn.LeakyReLU(),\n",
        "                torch.nn.Linear(1024, 512),\n",
        "                torch.nn.LeakyReLU(),\n",
        "                torch.nn.Linear(512, NUM_CLASSES)\n",
        "            )\n",
        "- The optimizer can be changed in the \"Define Model\" section\n",
        "  - Examples:\n",
        "    - `optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)`\n",
        "    - `optimizer = torch.optim.Adam(model.parameters())`\n",
        "- An optional scheduler can be added in the \"Define Model\" section\n",
        "  - Example:\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                       mode='max',\n",
        "                                                       factor=LR_PLATEAU_FACTOR,\n",
        "                                                       threshold=LR_PLATEAU_THRESHOLD,\n",
        "                                                       patience=LR_PLATEAU_PATIENCE,\n",
        "                                                       verbose=True)\n",
        "    - `scheduler.step(valid_accuracy)` should be called every epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yViVAO4KByZ8",
        "colab_type": "text"
      },
      "source": [
        "#### Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXvoIDJOCuKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_PATH = '/content/drive/My Drive/combined_car_data'\n",
        "NUM_CLASSES = 196\n",
        "\n",
        "TRAIN_PERCENT = 0.8\n",
        "VALID_PERCENT = 0.1\n",
        "TEST_PERCENT = 0.1\n",
        "\n",
        "# hyperparameters\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 32\n",
<<<<<<< Updated upstream
        "# LEARNING_RATE = 0.1\n",
=======
        "LEARNING_RATE = 0.1\n",
>>>>>>> Stashed changes
        "# MOMENTUM = 0.9\n",
        "# LR_PLATEAU_FACTOR = 0.1\n",
        "# LR_PLATEAU_THRESHOLD = 0.99\n",
        "# LR_PLATEAU_PATIENCE = 0"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7Ton48rCBjN",
        "colab_type": "text"
      },
      "source": [
        "# Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLtrnlpu0ZcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a custom dataset for different transforms\n",
        "# https://discuss.pytorch.org/t/changing-transformation-applied-to-data-during-training/15671/5\n",
        "class CustomTransformDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, transform):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        image = self.dataset[index][0]\n",
        "        label = self.dataset[index][1]\n",
        "        return self.transform(image), label\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYMkzisKCDcK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load datasets using ImageFolder structure\n",
        "dataset = datasets.ImageFolder(DATA_PATH)\n",
        "\n",
        "# create train, validation, test split\n",
        "train_split = (int)(TRAIN_PERCENT * len(dataset))\n",
        "valid_split = (int)(VALID_PERCENT * len(dataset))\n",
        "test_split = (int)(TEST_PERCENT * len(dataset))\n",
        "leftover = len(dataset) - train_split - valid_split - test_split\n",
        "train_split += leftover\n",
        "train_data, valid_data, test_data = torch.utils.data.random_split(dataset, [train_split, valid_split, test_split])\n",
        "\n",
        "# set transforms\n",
        "train_data = CustomTransformDataset(train_data, process.DATA_AUGMENT_TRANSFORM)\n",
        "valid_data = CustomTransformDataset(valid_data, process.PREPROCESS_TRANSFORM)\n",
        "test_data = CustomTransformDataset(test_data, process.PREPROCESS_TRANSFORM)\n",
        "\n",
        "# create data loaders using datasets, set shuffle to true so epochs are different\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03uSRuJGCFTU",
        "colab_type": "text"
      },
      "source": [
        "# Set Up Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62kc6Y1vCH3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# transfer learning using pretrained model\n",
        "# considering densenet121, resnet34, resnet50, resnet101\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# freeze the model\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# define the last fully-connected layer to be a linear transformation from num_features to num_classes\n",
        "num_features = model.fc.in_features\n",
        "model.fc = torch.nn.Sequential(\n",
        "    torch.nn.Linear(num_features, 1024),\n",
        "    torch.nn.LeakyReLU(),\n",
        "    torch.nn.Linear(1024, 512),\n",
        "    torch.nn.LeakyReLU(),\n",
        "    torch.nn.Linear(512, NUM_CLASSES)\n",
        ")\n",
        "\n",
        "# define the criterion as the cross entropy loss function\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# use the gpu\n",
        "model.cuda()\n",
        "criterion.cuda()\n",
        "\n",
        "# define the stochastic gradient descent with the learning rate and momentum\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
<<<<<<< Updated upstream
        "optimizer = torch.optim.Adam(model.parameters())\n",
=======
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
>>>>>>> Stashed changes
        "\n",
        "# lower the learning rate as we stop improving, lr = lr * factor\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "#                                                        mode='max',\n",
        "#                                                        factor=LR_PLATEAU_FACTOR,\n",
        "#                                                        threshold=LR_PLATEAU_THRESHOLD,\n",
        "#                                                        patience=LR_PLATEAU_PATIENCE,\n",
        "#                                                        verbose=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PhzQpdgCJnz",
        "colab_type": "text"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJGQc1WoCMZl",
        "colab_type": "text"
      },
      "source": [
        "#### Validation/Training Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iNTYXoJCPOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run function for both training and validation\n",
        "def run(model, loader, criterion, train=False):\n",
        "    # switch to train mode or eval mode\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    losses = []\n",
        "    predictions_list = []\n",
        "    targets_list = []\n",
        "\n",
        "    # iterate over batches of validation data\n",
        "    for images, labels in tqdm(loader):\n",
        "        # move images and labels to gpu\n",
        "        images, labels = images.cuda(), labels.cuda()\n",
        "\n",
        "        if train:\n",
        "            # zero out accumulated parameter gradients before doing back propagation \n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model.forward(images)\n",
        "\n",
        "            # back propagate the loss from the loss function to each parameter\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                outputs = model.forward(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "  \n",
        "        # sum the losses\n",
        "        losses.append(criterion(outputs, labels).item())\n",
        "  \n",
        "        # get the probability tensor by undoing log of loss function\n",
        "        probability = torch.exp(outputs)\n",
        "\n",
        "        # get the predictions\n",
        "        predictions = probability.max(dim=1)[1]\n",
        "\n",
        "        predictions_list.append(predictions.cpu())\n",
        "        targets_list.append(labels.cpu().data)\n",
        "\n",
        "    accuracies = []\n",
        "    for idx in range(len(predictions_list)):\n",
        "        accuracies.append(accuracy_score(targets_list[idx], predictions_list[idx]))\n",
        "\n",
        "    accuracy = np.average(accuracies)\n",
        "    loss = np.average(losses)\n",
        "    \n",
        "    return loss, accuracy"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekCL-1-8CROP",
        "colab_type": "text"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3Xa2pLACS6s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "68889389-2846-435e-b1e0-a5588dcda39e"
      },
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_loss, train_accuracy = run(model, train_loader, criterion, train=True)\n",
        "    valid_loss, valid_accuracy = run(model, valid_loader, criterion)\n",
        "    \n",
        "    # print each epoch\n",
        "    print(f'\\n\\\n",
        "    Epoch #{epoch + 1}\\n\\\n",
        "    Training Loss: {train_loss : .3f}\\t\\\n",
        "    Training Accuracy: {train_accuracy * 100 : .3f}%\\n\\\n",
        "    Validation Loss: {valid_loss : .3f}\\t\\\n",
        "    Validation Accuracy: {valid_accuracy * 100 : .3f}%\\n')\n",
        "\n",
        "    # slow the learning rate according to validation accuracy\n",
        "    # scheduler.step(valid_accuracy)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 405/405 [08:03<00:00,  1.19s/it]\n",
            "100%|██████████| 51/51 [01:02<00:00,  1.23s/it]\n",
            "  0%|          | 0/405 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "    Epoch #1\n",
            "    Training Loss:  4.803\t    Training Accuracy:  2.292%\n",
            "    Validation Loss:  4.183\t    Validation Accuracy:  6.788%\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 405/405 [05:36<00:00,  1.20it/s]\n",
            "100%|██████████| 51/51 [00:40<00:00,  1.27it/s]\n",
            "  0%|          | 0/405 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "    Epoch #2\n",
            "    Training Loss:  3.976\t    Training Accuracy:  8.743%\n",
            "    Validation Loss:  3.710\t    Validation Accuracy:  12.255%\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 405/405 [05:27<00:00,  1.24it/s]\n",
            "100%|██████████| 51/51 [00:40<00:00,  1.27it/s]\n",
            "  0%|          | 0/405 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "    Epoch #3\n",
            "    Training Loss:  3.569\t    Training Accuracy:  14.392%\n",
            "    Validation Loss:  3.438\t    Validation Accuracy:  15.060%\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 89%|████████▊ | 359/405 [04:45<00:33,  1.39it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyNmg-UPCVCE",
        "colab_type": "text"
      },
      "source": [
        "# Test Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn9GRSDHCX3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_accuracy = (model, test_loader, criterion)\n",
        "\n",
        "print(f'Test accuracy of model: {test_accuracy}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEN_2G1vCaRs",
        "colab_type": "text"
      },
      "source": [
        "# Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBm0wnzTCcTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the feature weights state, new fully connected layer, class-to-index map, sgd state, and number of epochs\n",
        "checkpoint = {'state_dict': model.state_dict(),\n",
        "              'model': model.fc,\n",
        "              'class_to_idx': train_data.class_to_idx,\n",
        "              'opt_state': optimizer.state_dict,\n",
        "              'num_epochs': NUM_EPOCHS\n",
        "              }\n",
        "\n",
        "torch.save(checkpoint, 'checkpoint.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0iDvDIwa6dg",
        "colab_type": "text"
      },
      "source": [
        "# Sources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fKWUFqaa-Gu",
        "colab_type": "text"
      },
      "source": [
        "All code in this journal was adapted from these sources:\n",
        "- https://github.com/wengsengh/Car-Models-Classifier/blob/master/car_models_classifier.ipynb\n",
        "- https://www.analyticsvidhya.com/blog/2019/10/how-to-master-transfer-learning-using-pytorch/\n",
        "- https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/lr_scheduling/\n",
        "- https://towardsdatascience.com/a-beginners-tutorial-on-building-an-ai-image-classifier-using-pytorch-6f85cb69cba7"
      ]
    }
  ]
}